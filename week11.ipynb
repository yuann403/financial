{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN1ToEgvcsti4jIoPS4DzP2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuann403/financial/blob/main/week11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 設計一個多模態模型，採用**(a)早期融合**、(b)晚期融合或(c)中期融合的方式進行數據整合（擇一實現）。\n",
        "\n",
        "* 多模態資料來源可包括以下組合：\n",
        "    **新聞情緒指標 + 股價資料**\n",
        "\n",
        "* 模型目標可針對**分類任務**（如股價漲跌預測）"
      ],
      "metadata": {
        "id": "kv0lnKYgXdPl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "新聞+股價資料集：https://www.kaggle.com/competitions/stock-market-prediction-and-sentimental-analysis/overview"
      ],
      "metadata": {
        "id": "W_EEvIoPZpIY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KwKOu5opR-Qy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 加載數據\n",
        "news_data = pd.read_csv('/content/Combined_News_DJIA(train).csv')\n",
        "stock_data = pd.read_csv('/content/DJIA_table(train).csv')\n",
        "\n",
        "# # 查看數據結構\n",
        "# print(\"新聞數據：\")\n",
        "# print(news_data.head())\n",
        "# print(news_data.info())\n",
        "\n",
        "# print(\"股價數據：\")\n",
        "# print(stock_data.head())\n",
        "# print(stock_data.info())"
      ],
      "metadata": {
        "collapsed": true,
        "id": "a4TvWbrk0dge"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 假設需要合併的欄位是 'Top1', 'Top2', 'Top3', ..., 'TopN'\n",
        "news_columns = [col for col in news_data.columns if 'Top' in col]  # 自動檢索包含 'Top' 的列名\n",
        "print(\"需要合併的欄位：\", news_columns)\n",
        "\n",
        "# 合併欄位\n",
        "news_data['News'] = news_data[news_columns].fillna('').agg(' '.join, axis=1)\n",
        "\n",
        "# 查看新的數據結構\n",
        "print(news_data[['News']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vroMshhU63H1",
        "outputId": "e5f5e11e-e8fc-4e1e-f863-29ff46b1fe75"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "需要合併的欄位： ['Top1', 'Top2', 'Top3', 'Top4', 'Top5', 'Top6', 'Top7', 'Top8', 'Top9', 'Top10', 'Top11', 'Top12', 'Top13', 'Top14', 'Top15', 'Top16', 'Top17', 'Top18', 'Top19', 'Top20', 'Top21', 'Top22', 'Top23', 'Top24', 'Top25']\n",
            "                                                News\n",
            "0  b\"Georgia 'downs two Russian warplanes' as cou...\n",
            "1  b'Why wont America and Nato help us? If they w...\n",
            "2  b'Remember that adorable 9-year-old who sang a...\n",
            "3  b' U.S. refuses Israel weapons to attack Iran:...\n",
            "4  b'All the experts admit that we should legalis...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 添加移動平均線\n",
        "stock_data['SMA_5'] = stock_data['Close'].rolling(window=5).mean()\n",
        "stock_data['SMA_10'] = stock_data['Close'].rolling(window=10).mean()\n",
        "\n",
        "# 計算 RSI\n",
        "def calculate_rsi(data, window=14):\n",
        "    delta = data['Close'].diff(1)\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
        "    rs = gain / loss\n",
        "    rsi = 100 - (100 / (1 + rs))\n",
        "    return rsi\n",
        "\n",
        "stock_data['RSI'] = calculate_rsi(stock_data)"
      ],
      "metadata": {
        "id": "ON9Bu8gVZDQM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 統一日期格式\n",
        "news_data['Date'] = pd.to_datetime(news_data['Date'])\n",
        "stock_data['Date'] = pd.to_datetime(stock_data['Date'])\n",
        "\n",
        "# 處理空值和重複值\n",
        "news_data = news_data.drop_duplicates(subset='Date').dropna(subset=['Date'])\n",
        "stock_data = stock_data.drop_duplicates(subset='Date').dropna(subset=['Date'])\n",
        "\n",
        "# 過濾日期範圍的交集\n",
        "common_dates = set(news_data['Date']).intersection(set(stock_data['Date']))\n",
        "news_data = news_data[news_data['Date'].isin(common_dates)]\n",
        "stock_data = stock_data[stock_data['Date'].isin(common_dates)]\n",
        "\n",
        "# 合併數據\n",
        "merged_data = pd.merge(news_data, stock_data, on='Date', how='inner')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMKh5HfgCmm8",
        "outputId": "1a7a31b0-3919-4cd9-b8eb-791dfc92fe4d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-bc725ca3bb73>:3: UserWarning: Parsing dates in %d-%m-%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
            "  stock_data['Date'] = pd.to_datetime(stock_data['Date'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 確認需要刪除的欄位\n",
        "columns_to_drop = [col for col in merged_data.columns if col.startswith('Top')]\n",
        "\n",
        "# 刪除欄位\n",
        "merged_data = merged_data.drop(columns=columns_to_drop, axis=1)\n",
        "\n",
        "# 查看刪除後的數據\n",
        "print(\"刪除後的數據:\")\n",
        "print(merged_data.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsfdANQFA2SV",
        "outputId": "715c2861-971f-4fb4-d18e-0ff2e27eb70b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "刪除後的數據:\n",
            "Index(['Date', 'Label', 'News', 'Open', 'High', 'Low', 'Close', 'Volume',\n",
            "       'Adj Close', 'SMA_5', 'SMA_10', 'RSI'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 提取特徵與標籤\n",
        "emotion_features = ['News']  # 假設新聞文本列名為 'News'\n",
        "stock_features = ['Close','RSI']  # 假設股價數據列\n",
        "label_column = 'Label'  # 假設分類標籤列名為 'Label'\n",
        "\n",
        "# 確認數據是否完整\n",
        "data = merged_data.dropna(subset=emotion_features + stock_features + [label_column])\n",
        "\n",
        "# 提取特徵與標籤\n",
        "news_texts = data[emotion_features[0]].values  # 新聞文本\n",
        "stock_data = data[stock_features].values  # 股價數據\n",
        "labels = data[label_column].values  # 分類標籤"
      ],
      "metadata": {
        "id": "uWs0Ue8J2F59"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 標準化股價數據\n",
        "scaler = StandardScaler()\n",
        "scaled_stock_data = scaler.fit_transform(stock_data)"
      ],
      "metadata": {
        "id": "AWBKn23j-rEz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 確保數據一致\n",
        "min_length = min(len(news_texts), len(scaled_stock_data), len(labels))\n",
        "\n",
        "news_texts = news_texts[:min_length]\n",
        "scaled_stock_data = scaled_stock_data[:min_length]\n",
        "labels = labels[:min_length]"
      ],
      "metadata": {
        "id": "JAKi2y-wD9XB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 加載 BERT 分詞器\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# 分詞新聞文本\n",
        "def tokenize_news(texts):\n",
        "    return tokenizer(list(texts), truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
        "\n",
        "tokenized_news = tokenize_news(news_texts)"
      ],
      "metadata": {
        "id": "OyVyiZlq1k_f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9057411-885d-4dd3-e74c-4a2f772c6cec"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"tokenized_news size: {len(tokenized_news['input_ids']) if 'input_ids' in tokenized_news else len(tokenized_news)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4xnYOYDDchR",
        "outputId": "d7b78c91-a219-499c-ab33-7ae94c849f86"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenized_news size: 1850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 確保 tokenized_news 的樣本數一致\n",
        "print(f\"Tokenized news samples: {len(tokenized_news['input_ids'])}\")\n",
        "print(f\"Stock data samples: {len(scaled_stock_data)}\")\n",
        "print(f\"Labels samples: {len(labels)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBm4S-1AEHFE",
        "outputId": "ae318fe3-6de1-4bf0-b608-5786e4b28978"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized news samples: 1850\n",
            "Stock data samples: 1850\n",
            "Labels samples: 1850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenized_news['input_ids']\n",
        "attention_mask = tokenized_news['attention_mask']"
      ],
      "metadata": {
        "id": "a4wPkJMlFbpu"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 分割數據集\n",
        "X_train_ids, X_test_ids, X_train_mask, X_test_mask, X_train_stock, X_test_stock, y_train, y_test = train_test_split(\n",
        "    input_ids, attention_mask, scaled_stock_data, labels, test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "85YGxZsVAHIx"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MidFusionModel(nn.Module):\n",
        "    def __init__(self, bert_model_name, stock_feature_dim, hidden_dim, num_classes):\n",
        "        super(MidFusionModel, self).__init__()\n",
        "        # BERT 模型\n",
        "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
        "        # 股價特徵處理\n",
        "        self.stock_fc = nn.Sequential(\n",
        "            nn.Linear(stock_feature_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "        # 結合後的分類器\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(768 + hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(hidden_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, stock_features):\n",
        "        # BERT 特徵提取\n",
        "        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        text_features = bert_outputs.pooler_output\n",
        "\n",
        "        # 股價特徵提取\n",
        "        stock_features = self.stock_fc(stock_features)\n",
        "\n",
        "        # 拼接特徵\n",
        "        combined_features = torch.cat((text_features, stock_features), dim=1)\n",
        "        output = self.classifier(combined_features)\n",
        "        return output"
      ],
      "metadata": {
        "id": "0KkGDMoAESfO"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 構建 TensorDataset\n",
        "train_dataset = TensorDataset(\n",
        "    torch.tensor(X_train_ids, dtype=torch.long),\n",
        "    torch.tensor(X_train_mask, dtype=torch.long),\n",
        "    torch.tensor(X_train_stock, dtype=torch.float32),\n",
        "    torch.tensor(y_train, dtype=torch.long)\n",
        ")\n",
        "\n",
        "test_dataset = TensorDataset(\n",
        "    torch.tensor(X_test_ids, dtype=torch.long),\n",
        "    torch.tensor(X_test_mask, dtype=torch.long),\n",
        "    torch.tensor(X_test_stock, dtype=torch.float32),\n",
        "    torch.tensor(y_test, dtype=torch.long)\n",
        ")\n",
        "\n",
        "# 創建 DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRTKFYUxFjqf",
        "outputId": "dcea24fb-959e-4d03-d8bc-86e3cbd4c332"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-679dbc710611>:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  torch.tensor(X_train_ids, dtype=torch.long),\n",
            "<ipython-input-16-679dbc710611>:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  torch.tensor(X_train_mask, dtype=torch.long),\n",
            "<ipython-input-16-679dbc710611>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  torch.tensor(X_test_ids, dtype=torch.long),\n",
            "<ipython-input-16-679dbc710611>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  torch.tensor(X_test_mask, dtype=torch.long),\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 初始化模型\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MidFusionModel(\"bert-base-uncased\", stock_feature_dim=X_train_stock.shape[1], hidden_dim=128, num_classes=2)\n",
        "model.to(device)\n",
        "\n",
        "# 訓練過程\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "scheduler = StepLR(optimizer, step_size=5, gamma=0.5)  # 每5個epoch將學習率減半\n",
        "\n",
        "epochs = 20\n",
        "for epoch in range(epochs):\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "  for input_ids, attention_mask, stock_features, labels in train_loader:\n",
        "    input_ids, attention_mask, stock_features, labels = (\n",
        "        input_ids.to(device), attention_mask.to(device), stock_features.to(device), labels.to(device)\n",
        "    )\n",
        "    outputs = model(input_ids, attention_mask, stock_features)\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "  scheduler.step()  # 調整學習率\n",
        "  print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naR1YYM5Ellh",
        "outputId": "9ef36b11-566c-4c96-c729-30eeedacc004"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 32.8632\n",
            "Epoch 2/20, Loss: 32.9233\n",
            "Epoch 3/20, Loss: 32.7194\n",
            "Epoch 4/20, Loss: 32.6973\n",
            "Epoch 5/20, Loss: 32.7677\n",
            "Epoch 6/20, Loss: 32.7089\n",
            "Epoch 7/20, Loss: 32.7026\n",
            "Epoch 8/20, Loss: 32.6130\n",
            "Epoch 9/20, Loss: 32.6122\n",
            "Epoch 10/20, Loss: 32.5617\n",
            "Epoch 11/20, Loss: 32.5570\n",
            "Epoch 12/20, Loss: 32.5387\n",
            "Epoch 13/20, Loss: 32.5325\n",
            "Epoch 14/20, Loss: 32.5015\n",
            "Epoch 15/20, Loss: 32.5647\n",
            "Epoch 16/20, Loss: 32.5407\n",
            "Epoch 17/20, Loss: 32.5021\n",
            "Epoch 18/20, Loss: 32.5392\n",
            "Epoch 19/20, Loss: 32.5187\n",
            "Epoch 20/20, Loss: 32.4878\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 測試模型\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "with torch.no_grad():\n",
        "    for input_ids, attention_mask, stock_features, labels in test_loader:\n",
        "        input_ids, attention_mask, stock_features, labels = (\n",
        "            input_ids.to(device), attention_mask.to(device), stock_features.to(device), labels.to(device)\n",
        "        )\n",
        "        outputs = model(input_ids, attention_mask, stock_features)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# 評估\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(all_labels, all_preds, target_names=[\"Down\", \"Up\"]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ftz88eAF4MZ",
        "outputId": "17135766-17ce-4f9f-99c8-5ee0429a1b6e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Down       0.00      0.00      0.00       156\n",
            "          Up       0.58      1.00      0.73       214\n",
            "\n",
            "    accuracy                           0.58       370\n",
            "   macro avg       0.29      0.50      0.37       370\n",
            "weighted avg       0.33      0.58      0.42       370\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 計算準確度\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "print(f\"模型準確度: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dS1sEIDDHfaE",
        "outputId": "67d22828-c398-47a2-8bea-8a104da1f0aa"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "模型準確度: 0.58\n"
          ]
        }
      ]
    }
  ]
}